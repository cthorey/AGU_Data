{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "import time,pickle\n",
    "from tqdm import *\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this notebook is to find similar papers from the database based\n",
    "on their abstract and their title.\n",
    "\n",
    "The following is greatly inspired from [Amir Amini](https://www.kaggle.com/amirhamini/d/benhamner/nips-2015-papers/find-similar-papers-knn/notebook) and [brandonmrose](http://brandonrose.org/clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts used to scrap the [AGU wesbsite](https://fallmeeting.agu.org/2015/) as well as the resulting data are stored on this [repo](https://github.com/cthorey/agu_data) if you want to reproduce the following by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "home = expanduser('~')\n",
    "os.chdir(os.path.join(home,'Documents','repos','agu_data','agu_data'))\n",
    "from Data_Utils import *\n",
    "\n",
    "data = get_all_data('agu2015')\n",
    "abstracts = [df.abstract for df in data if (df.title != '') and (df.abstract != '')]\n",
    "titles = [df.title for df in data if (df.title != '') and (df.abstract != '')]\n",
    "links = [df.link for df in data if (df.title != '') and (df.abstract != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGU abstract are short, $\\sim 300$ words and looks like that"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, some of them are empty which more likely corresponds to papers that have been retracted before the beginning of the meeting. We also have to:\n",
    "\n",
    "- Make everything lower case\n",
    "- Remove all strange character, convert unicode\n",
    "- replace \\n by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    ''' function to clean each abstract/title'''\n",
    "    \n",
    "    if text.split('\\n')[0].split(' ')[0] =='ePoster':\n",
    "        text = ' '.join(text.split('\\n')[1:])\n",
    "    list_of_cleaning_signs = ['\\x0c', '\\n']\n",
    "    for sign in list_of_cleaning_signs:\n",
    "        text = text.replace(sign, ' ')\n",
    "    #text = unicode(text, errors='ignore')\n",
    "    clean_text = re.sub('[^a-zA-Z]+', ' ', text)\n",
    "    return clean_text.lower().strip()\n",
    "\n",
    "papers = [clean_text(df.abstract) for df in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    tokens = set(reduce(lambda x,y:x+y,[nltk.word_tokenize(clean_text(sent)) \n",
    "                                        for sent in nltk.sent_tokenize(text)]))\n",
    "    stems = [stemmer.stem(t) for t in tokens]\n",
    "    return stems\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer_abstract = TfidfVectorizer(max_df=0.95, \n",
    "                                            max_features=200000, \n",
    "                                            min_df=0.05, \n",
    "                                            stop_words='english',\n",
    "                                            use_idf=True, \n",
    "                                            tokenizer=tokenize_and_stem,\n",
    "                                            lowercase = True,\n",
    "                                            ngram_range=(1,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 48s, sys: 4.14 s, total: 3min 52s\n",
      "Wall time: 3min 52s\n"
     ]
    }
   ],
   "source": [
    "%time tfidf_matrix_Abstract = tfidf_vectorizer_abstract.fit_transform(abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "terms_Abstract = tfidf_vectorizer_abstract.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def top_tfidf_feats(row, terms, top_n=25):\n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(terms[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df['feature']\n",
    "def given_link_give_keywords(tfidfMatrix, terms, paper_link, top_n=20):\n",
    "    row_id = links.index(paper_link)\n",
    "    row = np.squeeze(tfidfMatrix[row_id].toarray())\n",
    "    return top_tfidf_feats(row, terms, top_n)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keywords based on Abstract:\n",
      "0       melt\n",
      "1    product\n",
      "2      locat\n",
      "3     extent\n",
      "4      volum\n",
      "5      short\n",
      "6     center\n",
      "7        age\n",
      "8     nation\n",
      "9    dataset\n",
      "Name: feature, dtype: object\n"
     ]
    }
   ],
   "source": [
    "paper_id_example = links[159]\n",
    "print (\"Keywords based on Abstract:\")\n",
    "print (given_link_give_keywords(tfidf_matrix_Abstract,\n",
    "                                terms_Abstract, \n",
    "                                paper_id_example, \n",
    "                                top_n = 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "# Based on Abstract\n",
    "num_neighbors = 4\n",
    "nbrs_Abstract = NearestNeighbors(n_neighbors=num_neighbors,\n",
    "                                 algorithm='auto').fit(tfidf_matrix_Abstract)\n",
    "distances_Abstract, indices_Abstract = nbrs_Abstract.kneighbors(tfidf_matrix_Abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print (\"Nbrs of the example paper based on Abstract similarity: %r\" % indices_Abstract[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-85-495524424dc4>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-85-495524424dc4>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    tnltk.word_tokenize(f) for f in a]\u001b[0m\n\u001b[0m                             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "a = [clean_text(f) for f in nltk.sent_tokenize(abstracts[0])]\n",
    "tnltk.word_tokenize(f) for f in a\n",
    "tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'a weakening of the tropical tropospheric circulation has been inferred from historical observations and model projections but recent satellite based trends in surface wind speed precipitation and evaporation offer a conflicting view here this apparent contradiction is reconciled through consideration of sea surface temperature sst pattern effects and differences between tropospheric and surface winds the sst patterns are found to exert a strong influence on the surface winds acting against the intrinsic large scale circulation slow down to produce a near zero surface wind speed change averaged in space the intrinsic slow down and sst pattern effects combine to maintain a muted precipitation response despite the near zero change in surface wind speed because the planetary boundary layer is decoupled from the free troposphere the surface wind speed change cannot be regarded as an indicator for the trend of the tropical tropospheric circulation as a result there is no inconsistency between observed changes in surface winds and future projections of the atmospheric circulation the total tropospheric circulation dominated by the zonal wind e g walker cell tends to slow down with global warming while the meridional circulation i e hadley cell and total surface winds are by no means predicted to weaken robustly '"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "papers[200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "pathdata = os.path.join(root,'Data')\n",
    "papers_list = [f for f in os.listdir(pathdata) if f.split('_')[0]=='agu2015']\n",
    "papers = []\n",
    "errors = []\n",
    "for i in tqdm(range(len(papers_list))):\n",
    "    name = papers_list[i]\n",
    "    with open(os.path.join(pathdata,name), 'rb') as f:\n",
    "        idxs = pickle.load(f)\n",
    "        papers += idxs['papers']\n",
    "        errors += idxs['error']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.parse.stanford import StanfordParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "Could not find stanford-parser.jar jar file at stanford-parser-3.4.1-models.jar",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-ed8dd139c095>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mroot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'/Users/thorey/Documents/Tool/standfordParser/stanford-parser-full-2014-08-27'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0menglish_parser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStanfordParser\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'stanford-parser-3.4.1-models.jar'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/parse/stanford.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_to_jar, path_to_models_jar, model_path, encoding, verbose, java_options)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0menv_vars\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'STANFORD_PARSER'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m             \u001b[0msearchpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0m_stanford_url\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         )\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    661\u001b[0m         searchpath=(), url=None, verbose=True, is_regex=False):\n\u001b[1;32m    662\u001b[0m     return next(find_jar_iter(name_pattern, path_to_jar, env_vars,\n\u001b[0;32m--> 663\u001b[0;31m                          searchpath, url, verbose, is_regex))\n\u001b[0m\u001b[1;32m    664\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    665\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_decode_stdoutdata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstdoutdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/local/Library/Frameworks/Python.framework/Versions/2.7/lib/python2.7/site-packages/nltk/__init__.pyc\u001b[0m in \u001b[0;36mfind_jar_iter\u001b[0;34m(name_pattern, path_to_jar, env_vars, searchpath, url, verbose, is_regex)\u001b[0m\n\u001b[1;32m    577\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             raise LookupError('Could not find %s jar file at %s' %\n\u001b[0;32m--> 579\u001b[0;31m                             (name_pattern, path_to_jar))\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0;31m# Check environment variables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: Could not find stanford-parser.jar jar file at stanford-parser-3.4.1-models.jar"
     ]
    }
   ],
   "source": [
    "root = '/Users/thorey/Documents/Tool/standfordParser/stanford-parser-full-2014-08-27'\n",
    "english_parser = StanfordParser( os.path.join('stanford-parser-3.4.1-models.jar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name NERTagger",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-b9f563ac4065>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtag\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstanford\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNERTagger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name NERTagger"
     ]
    }
   ],
   "source": [
    "from nltk.tag.stanford import NERTagger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('c', 'NN'),\n",
       " ('h', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('c', 'NN'),\n",
       " ('o', 'NN'),\n",
       " ('l', 'NN'),\n",
       " ('a', 'DT'),\n",
       " ('t', 'NN'),\n",
       " ('e', 'NN')]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper = papers[0]\n",
    "lsttag = pos_tag(word_tokenize(paper.abstract))\n",
    "#zip(range(len(lsttag)),lsttag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Friday, 18 December 2015'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paper.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({u'%': 1,\n",
       "         u'(': 3,\n",
       "         u')': 3,\n",
       "         u',': 8,\n",
       "         u'.': 10,\n",
       "         u'3-hour': 2,\n",
       "         u'3-hourly': 1,\n",
       "         u'40': 1,\n",
       "         u'Concentration': 1,\n",
       "         u'Europe': 3,\n",
       "         u'Heavy': 1,\n",
       "         u'It': 2,\n",
       "         u'On': 1,\n",
       "         u'Over': 1,\n",
       "         u'Pathway': 1,\n",
       "         u'RCP8.5': 1,\n",
       "         u'Representative': 1,\n",
       "         u'Spain': 1,\n",
       "         u'The': 2,\n",
       "         u'There': 1,\n",
       "         u'This': 1,\n",
       "         u'While': 1,\n",
       "         u'a': 4,\n",
       "         u'an': 1,\n",
       "         u'analysed': 1,\n",
       "         u'analyses': 1,\n",
       "         u'and': 3,\n",
       "         u'are': 3,\n",
       "         u'as': 1,\n",
       "         u'at': 7,\n",
       "         u'be': 1,\n",
       "         u'behaviour': 1,\n",
       "         u'century': 1,\n",
       "         u'changes': 1,\n",
       "         u'changing': 1,\n",
       "         u'climate': 2,\n",
       "         u'comparing': 1,\n",
       "         u'concentrations': 1,\n",
       "         u'considered': 1,\n",
       "         u'daily': 6,\n",
       "         u'differences': 3,\n",
       "         u'distributions': 1,\n",
       "         u'established': 1,\n",
       "         u'events': 3,\n",
       "         u'examine': 1,\n",
       "         u'exhibit': 1,\n",
       "         u'exhibits': 1,\n",
       "         u'extrapolation': 1,\n",
       "         u'extreme': 1,\n",
       "         u'finding': 1,\n",
       "         u'focus': 1,\n",
       "         u'frequency': 1,\n",
       "         u'gas': 1,\n",
       "         u'given': 1,\n",
       "         u'goal': 1,\n",
       "         u'greenhouse': 1,\n",
       "         u'hand': 1,\n",
       "         u'has': 1,\n",
       "         u'hazard': 1,\n",
       "         u'high': 1,\n",
       "         u'how': 1,\n",
       "         u'however': 1,\n",
       "         u'illustrative': 1,\n",
       "         u'impacts': 1,\n",
       "         u'implications': 1,\n",
       "         u'in': 4,\n",
       "         u'increase': 1,\n",
       "         u'independent': 1,\n",
       "         u'indicate': 1,\n",
       "         u'intense': 3,\n",
       "         u'intensification': 3,\n",
       "         u'intensity': 1,\n",
       "         u'is': 5,\n",
       "         u'lower': 1,\n",
       "         u'main': 1,\n",
       "         u'major': 1,\n",
       "         u'mean': 1,\n",
       "         u'model': 2,\n",
       "         u'more': 1,\n",
       "         u'north-eastern': 1,\n",
       "         u'of': 7,\n",
       "         u'on': 2,\n",
       "         u'one': 1,\n",
       "         u'opposite': 1,\n",
       "         u'other': 1,\n",
       "         u'over': 3,\n",
       "         u'performed': 1,\n",
       "         u'possible': 1,\n",
       "         u'precipitation': 7,\n",
       "         u'present': 1,\n",
       "         u'projected': 3,\n",
       "         u'projections': 2,\n",
       "         u'properties': 1,\n",
       "         u'rainfall': 1,\n",
       "         u'rate': 1,\n",
       "         u'rather': 1,\n",
       "         u'regional': 1,\n",
       "         u'results': 1,\n",
       "         u'scale': 5,\n",
       "         u'scales': 3,\n",
       "         u'seaboard': 1,\n",
       "         u'significant': 1,\n",
       "         u'state-of-the-art': 1,\n",
       "         u'statistically': 1,\n",
       "         u'still': 1,\n",
       "         u'stronger': 1,\n",
       "         u'study': 1,\n",
       "         u'sub-daily': 3,\n",
       "         u'tendency': 1,\n",
       "         u'than': 2,\n",
       "         u'that': 1,\n",
       "         u'the': 16,\n",
       "         u'this': 2,\n",
       "         u'time': 4,\n",
       "         u'to': 2,\n",
       "         u'towards': 1,\n",
       "         u'translates': 1,\n",
       "         u'uncertain': 1,\n",
       "         u'up': 1,\n",
       "         u'using': 1,\n",
       "         u'usually': 1,\n",
       "         u'well': 1,\n",
       "         u'western': 1,\n",
       "         u'when': 1,\n",
       "         u'will': 1,\n",
       "         u'with': 1})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.FreqDist(word_tokenize(paper.abstract))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = { 'a' : {'b':'c'},'d':{'e':'f'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = json.dumps(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"a\": {\"b\": \"c\"}, \"d\": {\"e\": \"f\"}}'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('data.txt', 'w') as outfile:\n",
    "    json.dump(b, outfile, sort_keys = True, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "b = {'paper1':{'abstract':paper.abstract,'title':paper.title,'authors':paper.author},\n",
    "     'paper2':{'abstract':paper.abstract,'title':paper.title,'authors':paper.author}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "    import codecs, json\n",
    "    with codecs.open('data.json', 'w', 'utf8') as outfile:\n",
    "         json.dump(b, outfile,sort_keys = True, indent = 4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open('data.json','r','utf8') as f :\n",
    "    a =json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'paper1', u'paper2']"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
