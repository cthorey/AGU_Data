{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os,re\n",
    "import time,pickle\n",
    "from tqdm import *\n",
    "from os.path import expanduser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The AGU conference is hold each year in San Francisco a few weeks before christmas.\n",
    "\n",
    "With nearly 24,000 attendees, AGU Fall Meeting is the largest Earth and space science meeting in the world. As such, it is also a great dataset to study trends in the geoscience community. In the following, I use the papers contribution in the year 2015 and 2014 to find out some hidden structure in the abstracts.\n",
    "The aim is two fold:\n",
    "\n",
    "- For a given contribution, identify a list of similar contribution in the database\n",
    "- For the contributions of a particular authors, identify a list of potential collaborators with names, adress and institute displayed on a map.\n",
    "\n",
    "The method is greatly inspired from [Amir Amini](https://www.kaggle.com/amirhamini/d/benhamner/nips-2015-papers/find-similar-papers-knn/notebook) and [brandonmrose](http://brandonrose.org/clustering)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scrapping the AGU website "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scripts used to scrap the [AGU wesbsite](https://fallmeeting.agu.org/2015/) as well as the resulting data are stored on this [repo](https://github.com/cthorey/agu_data) if you want to reproduce the following by yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "home = expanduser('~')\n",
    "os.chdir(os.path.join(home,'Documents','repos','agu_data','agu_data'))\n",
    "from Data_Utils import *\n",
    "\n",
    "data = get_all_data('agu2015')\n",
    "abstracts = [df.abstract for df in data if (df.title != [\"\"]) and (df.abstract != '')]\n",
    "titles = [' '.join(df.title) for df in data if (''.join(df.title) != \"\") and (df.abstract != '')]\n",
    "sources = [df for df in data if (df.title != [\"\"]) and (df.abstract != '')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AGU abstract are short, $\\sim 300$ words and looks like that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The title of this paper is : \n",
      "  Slow Climate Velocities in Mountain Streams Impart Thermal Resistance to Cold-Water Refugia Across the West\n",
      "\n",
      " The corresponding abstract is :\n",
      " ePoster - PA31C-2165_JHPark_AGU.pdf\n",
      "The purpose of the study is to find small greens’ disposition, types and sizes to reduce air temperature effectively in urban blocks. The research sites were six high developed blocks in Seoul, Korea. Air temperature was measured with mobile loggers in clear daytime during summer, from August to September, at screen level. Also the measurement repeated over three times a day during three days by walking and circulating around the experimental blocks and the control blocks at the same time. By analyzing spatial characteristics, the averaged air temperatures were classified with three spaces, sunny spaces, building-shaded spaces and small green spaces by using Kruskal-Wallis Test; and small green spaces in 6 blocks were classified into their outward forms, polygonal or linear and single or mixed. The polygonal and mixed types of small green spaces mitigated averaged air temperature of each block which they belonged with a simple linear regression model with adjusted R2 = 0.90**. As the area and volume of these types increased, the effect of air temperature reduction (ΔT; Air temperature difference between sunny space and green space in a block) also increased in a linear relationship. The experimental range of this research is 100m2 ~ 2,000m2 of area, and 1,000m3 ~ 10,000m3 of volume of small green space. As a result, more than 300m2 and 2,300m3 of polygonal green spaces with mixed vegetation is required to lower 1°C; 650m2 and 5,000m3 of them to lower 2°C; about 2,000m2 and about 10,000m3 of them to lower 4°C air temperature reduction in an urban block.\n"
     ]
    }
   ],
   "source": [
    "idx = 300\n",
    "print '\\n The title of this paper is : \\n %s'%(titles[idx])\n",
    "print '\\n The corresponding abstract is :\\n %s'%(abstracts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first remove extra text in the abstract (eposter) and (Invited) + convert unicode characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import unicodedata\n",
    "def clean_title(text):\n",
    "    if text.split(' ')[-1] == '(Invited)':\n",
    "        text = ' '.join(text.split(' ')[:-1])\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii','ignore')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "def clean_abstract(text):\n",
    "    if text.split('\\n')[0].split(' ')[0] =='ePoster':\n",
    "        text = ' '.join(text.split('\\n')[1:])\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii','ignore')\n",
    "    text = text.replace('\\n', ' ')\n",
    "    return text\n",
    "\n",
    "titles = map(clean_title,titles)\n",
    "abstracts = map(clean_abstract,abstracts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " The title of this paper is : \n",
      "  Slow Climate Velocities in Mountain Streams Impart Thermal Resistance to Cold-Water Refugia Across the West\n",
      "\n",
      " The corresponding abstract is :\n",
      " The purpose of the study is to find small greens disposition, types and sizes to reduce air temperature effectively in urban blocks. The research sites were six high developed blocks in Seoul, Korea. Air temperature was measured with mobile loggers in clear daytime during summer, from August to September, at screen level. Also the measurement repeated over three times a day during three days by walking and circulating around the experimental blocks and the control blocks at the same time. By analyzing spatial characteristics, the averaged air temperatures were classified with three spaces, sunny spaces, building-shaded spaces and small green spaces by using Kruskal-Wallis Test; and small green spaces in 6 blocks were classified into their outward forms, polygonal or linear and single or mixed. The polygonal and mixed types of small green spaces mitigated averaged air temperature of each block which they belonged with a simple linear regression model with adjusted R2 = 0.90**. As the area and volume of these types increased, the effect of air temperature reduction (T; Air temperature difference between sunny space and green space in a block) also increased in a linear relationship. The experimental range of this research is 100m2 ~ 2,000m2 of area, and 1,000m3 ~ 10,000m3 of volume of small green space. As a result, more than 300m2 and 2,300m3 of polygonal green spaces with mixed vegetation is required to lower 1C; 650m2 and 5,000m3 of them to lower 2C; about 2,000m2 and about 10,000m3 of them to lower 4C air temperature reduction in an urban block.\n"
     ]
    }
   ],
   "source": [
    "idx = 300\n",
    "print '\\n The title of this paper is : \\n %s'%(titles[idx])\n",
    "print '\\n The corresponding abstract is :\\n %s'%(abstracts[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizing and stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The texts above cannot be fed directly into an algorithm to identify pattern within the corpus.\n",
    "Instead, some form of vectorization have to be used.\n",
    "\n",
    "A bag of words method is generally used in that purpose. In such a method, each document is first broken into unbreakable tokens and, in this method:\n",
    "\n",
    "- each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "- the vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "In the following, I build a function based on the library **nltk** to broke a documents into its most basic form, a list of stems.\n",
    "\n",
    "To do so, \n",
    "\n",
    "- We first broke each documents in sentence and the words\n",
    "- Remove all numeric, ponctuations with the module **re**\n",
    "- Remove stowords from the tokens\n",
    "- Stem each resulting tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "# Load SnowballStemmer as variabled 'stemmer'\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "# load nltk's English stopwords as variable called 'stopwords'\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token.lower())\n",
    "    filtered_tokens = [token for token in filtered_tokens if token not in stopwords]\n",
    "    stems = map(stemmer.stem,filtered_tokens)\n",
    "    return map(str,stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf-idf representation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the **tokenize_and_stem** function, we can then break each document in tokens and vectorize it. The vectorization will result in a matrix whom \n",
    "\n",
    "- each line is a text document of the corpus\n",
    "- each column represent a token from the corpus\n",
    "- a specific value represent the number of occurences of a particular token in a particular document\n",
    "\n",
    "While **sklearn** provides a class **CountVectorizer** which can be used to form such a matrix, this representation often put to much weights on common words of the corpus, i.e. words like *the*, *a* in english. While this can be interesting in some situations, we would like to put more weight on words that make each abstract specific for our application.\n",
    "\n",
    "One way to do this is to use a **Tf-idf** normalization to re-weiht each value in the matrix by the frequency of the token in the whole corpus. **Tf** means term-frequency while **tf–idf** means term-frequency times inverse document-frequency.\n",
    "\n",
    "This normalization is implemented by the **TfidfTransformer** class of sklearn and is used in the following.\n",
    "\n",
    "Main parameters:\n",
    "\n",
    "- max_df : When building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold\n",
    "- min_df : When building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold\n",
    "- ngram_range : Range of n-gram to consider. Bi-gram and Three gram like floor fractured craters is preserved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small sub-routine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Small routine to save the idf class and the resulting matrix.\n",
    "model_saved = os.path.join(home,'Documents','repos','agu_data','Notebook','Models')\n",
    "from sklearn.externals import joblib\n",
    "def save_idf(folder,idf_vectorizer,idf_matrix):\n",
    "    joblib.dump(idf_vectorizer,os.path.join(folder,'idf_vectorizer.pkl'))\n",
    "    joblib.dump(idf_matrix,os.path.join(folder,'idf_matrix.pkl'))\n",
    "    \n",
    "def open_idf(folder):\n",
    "    idf_vectorizer = joblib.load(os.path.join(folder,'idf_vectorizer.pkl'))\n",
    "    idf_matrix = joblib.load(os.path.join(folder,'idf_matrix.pkl'))\n",
    "    return idf_vectorizer, idf_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Titles "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.7 s, sys: 327 ms, total: 20 s\n",
      "Wall time: 20.2 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set true if necessary to rebuild the model !\n",
    "fit_transform = True\n",
    "if fit_transform:\n",
    "    titles_tfidf_vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                                   max_df=0.95, \n",
    "                                   max_features=200000, \n",
    "                                   min_df=0.001, \n",
    "                                   stop_words=stopwords,\n",
    "                                   use_idf=True, \n",
    "                                   tokenizer=tokenize_and_stem,\n",
    "                                   lowercase = True,\n",
    "                                   ngram_range=(1,3))\n",
    "    %time idf_titles = titles_tfidf_vectorizer.fit_transform(titles)\n",
    "    folder_titles = os.path.join(model_saved,'tfidf','titles')\n",
    "    save_idf(folder_titles,titles_tfidf_vectorizer,idf_titles)\n",
    "else:\n",
    "    folder_title = os.path.join(model_saved,'tfidf','titles')\n",
    "    titles_tfidf_vectorizer, idf_titles = open_idf(folder_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstracts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5min 6s, sys: 5.96 s, total: 5min 11s\n",
      "Wall time: 5min 17s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Set true if necessary to rebuild the model !\n",
    "fit_transform = True\n",
    "if fit_transform:\n",
    "    abstracts_tfidf_vectorizer = TfidfVectorizer(analyzer = 'word',\n",
    "                                   max_df=0.9, \n",
    "                                   max_features=200000, \n",
    "                                   min_df=0.01, \n",
    "                                   stop_words=stopwords,\n",
    "                                   use_idf=True, \n",
    "                                   tokenizer=tokenize_and_stem,\n",
    "                                   lowercase = True,\n",
    "                                   ngram_range=(1,3))\n",
    "    %time idf_abstracts = abstracts_tfidf_vectorizer.fit_transform(abstracts)\n",
    "    folder_abstracts = os.path.join(model_saved,'tfidf','abstracts')\n",
    "    save_idf(folder_abstracts,abstracts_tfidf_vectorizer,idf_abstracts)\n",
    "else:\n",
    "    folder_abstracts = os.path.join(model_saved,'tfidf','abstracts')\n",
    "    %time abstracts_tfidf_vectorizer, idf_abstracts = open_idf(folder_abstracts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making sense of the tf-idf resulting arrays "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def top_tfidf_feats(row, features, top_n=25):\n",
    "    ''' Get top n tfidf values in row and return \n",
    "        them with their corresponding feature names.'''\n",
    "    \n",
    "    topn_ids = np.argsort(row)[::-1][:top_n]\n",
    "    top_feats = [(features[i], row[i]) for i in topn_ids]\n",
    "    df = pd.DataFrame(top_feats)\n",
    "    df.columns = ['feature', 'tfidf']\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.601231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>inner</td>\n",
       "      <td>0.352726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>magnetospher</td>\n",
       "      <td>0.337490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>two</td>\n",
       "      <td>0.173421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>moder</td>\n",
       "      <td>0.166026</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        feature     tfidf\n",
       "0         storm  0.601231\n",
       "1         inner  0.352726\n",
       "2  magnetospher  0.337490\n",
       "3           two  0.173421\n",
       "4         moder  0.166026"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_feats(np.squeeze(idf_abstracts[0].toarray()),abstracts_tfidf_vectorizer.get_feature_names(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>differ</td>\n",
       "      <td>0.531370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>storm</td>\n",
       "      <td>0.528002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>larg</td>\n",
       "      <td>0.519313</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global</td>\n",
       "      <td>0.411307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>zone observatori</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            feature     tfidf\n",
       "0            differ  0.531370\n",
       "1             storm  0.528002\n",
       "2              larg  0.519313\n",
       "3            global  0.411307\n",
       "4  zone observatori  0.000000"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_tfidf_feats(np.squeeze(idf_titles[0].toarray()),titles_tfidf_vectorizer.get_feature_names(),5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
